# -*- coding: utf-8 -*-
"""CloneMyPI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xccShMTPfraGeyiFZMogsceDCUqHHDUI

### **Clone My Professor**
This code helps researchers to clone the writing style of your PI / Professor, allowing you to reduce the number of revisions required to adjust the language.



*   Professors have their own writing styles
*   This pattern can be observed in their publications.
* When you approach them with a writing to be edited, naturally they will try revise it based on their style.
* This may require you multiple revisions to adapt to their style.
* what if their writing style can be learned by a locally running LLM and helps you rewrite it.
* This code helps you to do that

**Tech Stack**: ollama, pypdf, unsloath

**Read all the pdf's in a folder**


*   Load your professors publication into a folder named papers.
*   Remember: You will have to repeat it everytime you load.
"""

import os

# Get all files and directories in the current working directory
all_files = os.listdir('/content/papers/')

# Filter for files ending with '.pdf'
pdf_files = [file for file in all_files if file.endswith('.pdf')]

# Print the identified PDF files
print("Identified PDF files:", pdf_files)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# pip install PyPDF2

"""## Extract Text from PDFs

Iterate through the list of identified PDF files (`pdf_files`), open each file, and extract its text content page by page. Store the extracted text from each PDF in a list.

"""

from PyPDF2 import PdfReader

extracted_texts = []

for pdf_file_name in pdf_files:
    full_path = os.path.join('/content/papers/', pdf_file_name)
    current_pdf_text = []
    try:
        with open(full_path, 'rb') as f:
            reader = PdfReader(f)
            for page in reader.pages:
                current_pdf_text.append(page.extract_text())
        extracted_texts.append(" ".join(current_pdf_text)) # Join pages text for each PDF
        print(f"Successfully extracted text from {pdf_file_name}")
    except Exception as e:
        print(f"Error extracting text from {pdf_file_name}: {e}")

# Print the first 500 characters of the first extracted PDF text as an example
if extracted_texts:
    print("\nFirst 500 characters of the first extracted PDF text:")
    print(extracted_texts[0][:500])
else:
    print("No text was extracted.")

## Consolidate extracted text
all_combined_text = " ".join(extracted_texts)
print(f"\nTotal number of characters in all extracted text: {len(all_combined_text)}")

"""## Clean Combined Text with Regex

Use regular expressions to clean the `all_combined_text` by removing special characters, normalizing whitespace, and preparing it for further analysis.
"""

import re

def clean_text(text):
    # Remove the \xad character (soft hyphen)
    text = text.replace('\xad', '')
    # Replace multiple newlines with a single space
    text = re.sub(r'\n+', ' ', text)
    # Replace multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text)
    # Strip leading/trailing whitespace
    text = text.strip()
    return text

cleaned_combined_text = clean_text(all_combined_text)

print("First 1000 characters of the cleaned combined text:")
print(cleaned_combined_text[:1000])

print(f"\nTotal number of characters in the cleaned combined text: {len(cleaned_combined_text)}")

"""## Chunk Text into 2000 Character Segments

Divide the `cleaned_combined_text` into smaller segments, each containing a maximum of 2000 characters, to facilitate processing or analysis of smaller text blocks.
"""

def chunk_text(text, chunk_size):
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

chunk_size = 2000
text_chunks = chunk_text(cleaned_combined_text, chunk_size)

print(f"Total number of chunks: {len(text_chunks)}")
print("\nFirst chunk (2000 characters):")
print(text_chunks[0])

"""## Simplify Text Chunks using OpenAI

### Why are we doing this ?
The task we are trying to teach the LLM is
**Given a paragraph in layman language --> rewrite in the language of your professor**

You many have 100's of paragraphs to rewrite into layman language, this can be automated by OPENAI API.

The below paragraph show how it is done.

You will have to purchase API key 5$ will be more than enough for this task.

## Securely Provide OpenAI API Key

Securely provide your OpenAI API key using Colab's User Secrets feature.

### Instructions:
1. Click on the 'key' icon (ðŸ”‘) in the left sidebar of Google Colab to open the 'Secrets' panel.
2. Click 'Add new secret'.
3. For the **Name**, enter `OPENAI_API_KEY` (this exact name is important for the next step).
4. For the **Value**, paste your actual OpenAI API key.
5. Ensure 'Notebook access' is toggled ON for this notebook.

Once the secret is added, the next code block will retrieve and use it.
"""

import os
from openai import OpenAI

from google.colab import userdata
OPENAI_API_KEY = userdata.get('OPEN_API_KEY')

if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found. Please set it in Colab Secrets.")

# Initialize the OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY)

OPENAI_MODEL = "gpt-5-nano" # You can change this to a different OpenAI model if desired
simplified_chunks = []

print(f"Starting simplification of {len(text_chunks)} chunks using OpenAI model: {OPENAI_MODEL}\n")

for i, chunk in enumerate(text_chunks):
    try:
        prompt = f"Rewrite the following text in very simple words, like how a first draft for a non-expert audience would look like. Focus on clarity and brevity.\n\nText: {chunk}"
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that simplifies complex text."},
                {"role": "user", "content": prompt}
            ],
            )
        simplified_text = response.choices[0].message.content.strip()
        simplified_chunks.append(simplified_text)
        print(f"Successfully simplified chunk {i+1}/{len(text_chunks)}")

        # Print the first original and simplified chunk for verification
        if i == 0:
            print("\n--- Original First Chunk ---")
            print(chunk[:500]) # Print first 500 chars of original for brevity
            print("\n--- Simplified First Chunk (OpenAI) ---")
            print(simplified_text[:500]) # Print first 500 chars of simplified for brevity

    except Exception as e:
        print(f"Error simplifying chunk {i+1}: {e}")
        simplified_chunks.append(f"Error: {e}") # Append error message to maintain array length

print(f"\nTotal simplified chunks: {len(simplified_chunks)}")

import numpy as np
np.save('simple_chunks.npy',simplified_chunks )

len(simplified_chunks)

"""### Fine tuning LLM


*   Using unsloath for fine tuning
*   I had chosen Llama3.1, you can choose any of the available models.
* I'm depending on the LoRA method for fine tuning, you can also test QLoRA


"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os, re
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     import torch; v = re.match(r"[0-9]{1,}\.[0-9]{1,}", str(torch.__version__)).group(0)
#     xformers = "xformers==" + ("0.0.33.post1" if v=="2.9" else "0.0.32.post2" if v=="2.8" else "0.0.29.post3")
#     !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets==4.3.0" "huggingface_hub>=0.34.0" hf_transfer
#     !pip install --no-deps unsloth
# !pip install transformers==4.56.2
# !pip install --no-deps trl==0.22.2

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-1B-Instruct",# or choose "unsloth/Llama-3.2-3B-Instruct"
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

"""We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"""

model = FastLanguageModel.get_peft_model(
    model,
    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

"""<a name="Data"></a>
### Data Prep
We now use the `Llama-3.1` format for conversation style finetunes. We process ourdataset in ShareGPT style.
```
{"role": "system", "content": "You are an assistant"}
{"role": "user", "content": "What is 2+2?"}
{"role": "assistant", "content": "It's 4."}
```
"""

formatted_data = []

for i in range(len(simplified_chunks)):
    entry = [
        {"role": "system", "content": "You are an assistant who is helping to rewrite a given paragraph"},
        {"role": "user", "content": simplified_chunks[i]},
        {"role": "assistant", "content": text_chunks[i]}
    ]
    formatted_data.append(entry)

# Print the first entry to verify the format
print("First formatted data entry:")
print(formatted_data[0])

print(f"Total number of formatted entries: {len(formatted_data)}")

from unsloth.chat_templates import get_chat_template
from datasets import Dataset

dataset = []
for entry in formatted_data:
    simplified = tokenizer.apply_chat_template(entry, tokenize = False, add_generation_prompt = False)
    dataset.append({"conversations":entry,
                    "text":simplified})
dataset = Dataset.from_list(dataset)

dataset[5]["conversations"]

dataset[5]["text"]

"""<a name="Train"></a>
### Train the model
Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!
"""

from trl import SFTConfig, SFTTrainer
from transformers import DataCollatorForSeq2Seq
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    packing = False, # Can make training 5x faster for short sequences.
    args = SFTConfig(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 60,
        learning_rate = 2e-4,
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.001,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use TrackIO/WandB etc
    ),
)

"""We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."""

from unsloth.chat_templates import train_on_responses_only
trainer = train_on_responses_only(
    trainer,
    instruction_part = "<|start_header_id|>user<|end_header_id|>\n\n",
    response_part = "<|start_header_id|>assistant<|end_header_id|>\n\n",
)

tokenizer.decode(trainer.train_dataset[5]["input_ids"])

space = tokenizer(" ", add_special_tokens = False).input_ids[0]
tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5]["labels"]])

# @title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train()

""" ### Inferencing using text streamer

 You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!
"""

FastLanguageModel.for_inference(model) # Enable native 2x faster inference

messages = [
    {"role": "user", "content": "rewrite this sentence; The goal of this descriptive pilot study was to identify vocal tract configuration patterns and interactions between anatomical structures within and across vocally healthy CCM singers during the production of five industry-typical vocal qualities, including various belting qualities and a traditional/legit approach used in musical theater singing. The study of vocal tract configurations and differences in vocal tract shapes during CCM singing is necessary to shed light on the ongoing discussion on the production of belting voice qualities, to further describe the typical and distinct functions used in different belting qualities, and to subsequently identify atypical vocal tract adjustments that might contribute to phonotrauma-related injury"},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # Must add for generation
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,
                   use_cache = True, temperature = 1.5, min_p = 0.1)

"""### Results
* Input - rewrite this sentence; The goal of this descriptive pilot study was to identify vocal tract configuration patterns and interactions between anatomical structures within and across vocally healthy CCM singers during the production of five industry-typical vocal qualities, including various belting qualities and a traditional/legit approach used in musical theater singing. The study of vocal tract configurations and differences in vocal tract shapes during CCM singing is necessary to shed light on the ongoing discussion on the production of belting voice qualities, to further describe the typical and distinct functions used in different belting qualities, and to subsequently identify atypical vocal tract adjustments that might contribute to phonotrauma-related injury

* Output - in the vocally healthy CCM singer with the most diverse vocal qualities described during five industry-typical vocal qualities, including various belting qualities and a traditional/legit voice approach used in musical theater singing. Description of vocal tract configurations, interaction of anatomical structures within and across species, anatomical structures in and out of vocally healthy CCM singers During this descriptive pilot study, the goal was to identify vocal tract configurations and vocal tract shape interactions between anatomical structures of anatomical structures across different vocal qualities used in CCM singing during the production of five industry-typical vocal qualities, including various belting qualities, traditional/legit

<a name="Save"></a>
### Saving, loading finetuned models
To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.

**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!
"""

model.save_pretrained("lora_model")  # Local saving
tokenizer.save_pretrained("lora_model")

# This step requires more VRAM
#model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")